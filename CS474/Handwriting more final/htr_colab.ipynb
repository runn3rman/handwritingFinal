{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handwritten Text Recognition (PyTorch, Colab-ready)\n",
        "Lean, end-to-end notebook for IAM word-level HTR with CTC, augmentations, beam search, and Drive export.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings & hyperparameters\n",
        "IMAGE_SIZE = (160, 48)  # (width, height)\n",
        "BATCH_SIZE = 48        # smaller batch for bigger images\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "PAD_TOKEN = 99\n",
        "BLANK_TOKEN = 0  # for CTC loss\n",
        "SEED = 43\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install + download IAM via kagglehub (Colab-friendly)\n",
        "%pip install -q kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Set your Kaggle token (for Colab you can paste your token string or use /root/.kaggle/kaggle.json)\n",
        "os.environ['KAGGLE_API_TOKEN'] = 'KGAT_be9c382f7d5391aa23404b90f2c252cb'\n",
        "\n",
        "try:\n",
        "    print(\"Attempting to download dataset...\")\n",
        "    iam_root = Path(kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\"))\n",
        "    print(f\"Dataset downloaded to: {iam_root}\")\n",
        "except Exception as e:\n",
        "    print(f\"Download skipped/failed: {e}\")\n",
        "    print(\"Ensure KAGGLE_API_TOKEN is correct or upload ~/.kaggle/kaggle.json\")\n",
        "    iam_root = Path(\"/kaggle/input/iam-handwriting-word-database\")  # fallback for Kaggle runtimes\n",
        "\n",
        "DATA_INPUT_PATH = iam_root\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick peek at a few samples (optional)\n",
        "Visual sanity check; skip if you trust the paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_first_six_images(root_dir: Path):\n",
        "    words_files = list(root_dir.rglob('words.txt'))\n",
        "    if not words_files:\n",
        "        print(\"words.txt not found.\")\n",
        "        return\n",
        "    words_path = words_files[0]\n",
        "    a01_dirs = list(root_dir.rglob('a01'))\n",
        "    if not a01_dirs:\n",
        "        print(\"Could not find image directory 'a01'.\")\n",
        "        return\n",
        "    img_root = a01_dirs[0].parent\n",
        "\n",
        "    count = 0\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 5))\n",
        "    axes = axes.flatten()\n",
        "    with open(words_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('#') or not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split()\n",
        "            word_id = parts[0]\n",
        "            transcription = parts[-1]\n",
        "            id_parts = word_id.split('-')\n",
        "            folder1 = id_parts[0]\n",
        "            folder2 = f\"{id_parts[0]}-{id_parts[1]}\"\n",
        "            img_path = img_root / folder1 / folder2 / f\"{word_id}.png\"\n",
        "            if img_path.exists():\n",
        "                try:\n",
        "                    img = Image.open(img_path).convert(\"L\")\n",
        "                    ax = axes[count]\n",
        "                    ax.imshow(img, cmap=\"gray\")\n",
        "                    ax.set_title(f\"{transcription}\\n({word_id})\")\n",
        "                    ax.axis('off')\n",
        "                    count += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if count >= 6:\n",
        "                break\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_first_six_images(DATA_INPUT_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse IAM metadata and build charset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = []\n",
        "images_path = []\n",
        "\n",
        "words_file = DATA_INPUT_PATH / \"iam_words\" / \"words.txt\"\n",
        "assert words_file.exists(), f\"Missing words.txt at {words_file}\"\n",
        "\n",
        "characters = set()\n",
        "max_len = 0\n",
        "with words_file.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"#\") or not line.strip():\n",
        "            continue\n",
        "        parts = line.strip().split()\n",
        "        word_id = parts[0]\n",
        "        label = parts[-1]\n",
        "\n",
        "        first_folder = word_id.split(\"-\")[0]\n",
        "        second_folder = f\"{first_folder}-{word_id.split('-')[1]}\"\n",
        "        image_filename = f\"{word_id}.png\"\n",
        "        image_path = DATA_INPUT_PATH / \"iam_words\" / \"words\" / first_folder / second_folder / image_filename\n",
        "        if image_path.is_file() and image_path.stat().st_size > 0:\n",
        "            images_path.append(str(image_path))\n",
        "            labels.append(label)\n",
        "            characters.update(label)\n",
        "            max_len = max(max_len, len(label))\n",
        "\n",
        "characters = sorted(list(characters))\n",
        "print(f\"Total samples: {len(images_path)} | Max label len: {max_len}\")\n",
        "print(f\"Charset size: {len(characters)} -> {characters[:20]}{' ...' if len(characters) > 20 else ''}\")\n",
        "\n",
        "# Build lookup tables (0 reserved for blank)\n",
        "char_to_idx = {c: i + 1 for i, c in enumerate(characters)}\n",
        "idx_to_char = {i + 1: c for i, c in enumerate(characters)}\n",
        "num_classes = len(characters) + 1\n",
        "print(f\"num_classes (incl. blank): {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augmentations and preprocess transforms\n",
        "Stronger geometry + noise to improve robustness; val/test stay clean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomGaussianBlur:\n",
        "    def __init__(self, p=0.2, radius=(0.1, 1.2)):\n",
        "        self.p = p\n",
        "        self.radius = radius\n",
        "\n",
        "    def __call__(self, img: Image.Image):\n",
        "        if random.random() > self.p:\n",
        "            return img\n",
        "        r = random.uniform(self.radius[0], self.radius[1])\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=r))\n",
        "\n",
        "\n",
        "class AddGaussianNoise:\n",
        "    def __init__(self, p=0.25, sigma=(0.0, 0.08)):\n",
        "        self.p = p\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x: torch.Tensor):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        s = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        return (x + torch.randn_like(x) * s).clamp(-1.0, 1.0)\n",
        "\n",
        "\n",
        "def build_transforms(image_size_wh):\n",
        "    w, h = image_size_wh  # (W, H)\n",
        "    train_tf = T.Compose([\n",
        "        T.Grayscale(num_output_channels=1),\n",
        "        T.Resize((h, w)),\n",
        "        T.RandomApply([T.RandomAffine(\n",
        "            degrees=10,\n",
        "            translate=(0.06, 0.12),\n",
        "            scale=(0.85, 1.15),\n",
        "            shear=(-10, 10),\n",
        "            interpolation=T.InterpolationMode.BILINEAR,\n",
        "            fill=255,\n",
        "        )], p=0.75),\n",
        "        T.RandomApply([T.RandomPerspective(distortion_scale=0.20, p=1.0)], p=0.25),\n",
        "        T.RandomApply([RandomGaussianBlur(p=1.0)], p=0.25),\n",
        "        T.RandomApply([T.ColorJitter(brightness=0.25, contrast=0.35)], p=0.60),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5], [0.5]),\n",
        "        AddGaussianNoise(p=0.30, sigma=(0.0, 0.06)),\n",
        "        T.RandomErasing(p=0.20, scale=(0.01, 0.05), ratio=(0.2, 5.0), value=1.0),\n",
        "    ])\n",
        "\n",
        "    val_tf = T.Compose([\n",
        "        T.Grayscale(num_output_channels=1),\n",
        "        T.Resize((h, w)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5], [0.5]),\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "train_tf, val_tf = build_transforms(IMAGE_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset + collate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IAMWordDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        target = torch.tensor([char_to_idx[c] for c in label], dtype=torch.long)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n",
        "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN)\n",
        "    return images, targets_padded, target_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split + DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_imgs, test_imgs, train_labels, test_labels = train_test_split(\n",
        "    images_path, labels, test_size=0.2, random_state=SEED, stratify=None\n",
        ")\n",
        "val_imgs, test_imgs, val_labels, test_labels = train_test_split(\n",
        "    test_imgs, test_labels, test_size=0.5, random_state=SEED, stratify=None\n",
        ")\n",
        "\n",
        "train_ds = IAMWordDataset(train_imgs, train_labels, transform=train_tf)\n",
        "val_ds = IAMWordDataset(val_imgs, val_labels, transform=val_tf)\n",
        "test_ds = IAMWordDataset(test_imgs, test_labels, transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_batch)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model: CNN + BiLSTM + CTC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HTRModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "        rnn_input_size = 128 * (IMAGE_SIZE[1] // 8)\n",
        "        self.lstm1 = nn.LSTM(rnn_input_size, 256, bidirectional=True, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()  # [B, W, C, H]\n",
        "        x = x.view(b, w, c * h)                 # [B, W, C*H]\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.log_softmax(2)\n",
        "        return x\n",
        "\n",
        "model = HTRModel(num_classes=num_classes).to(DEVICE)\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training helpers (CTC, decoding, CER)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ctc_loss_fn = nn.CTCLoss(blank=BLANK_TOKEN, zero_infinity=True)\n",
        "\n",
        "\n",
        "def greedy_decode(log_probs, input_lengths):\n",
        "    max_probs = log_probs.detach().cpu().argmax(2)\n",
        "    decoded = []\n",
        "    for seq, seq_len in zip(max_probs, input_lengths):\n",
        "        prev = None\n",
        "        chars = []\n",
        "        for idx in seq[:seq_len]:\n",
        "            idx = idx.item()\n",
        "            if idx == BLANK_TOKEN:\n",
        "                prev = None\n",
        "                continue\n",
        "            if idx != prev:\n",
        "                chars.append(idx_to_char.get(idx, \"\"))\n",
        "            prev = idx\n",
        "        decoded.append(\"\".join(chars))\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    m, n = len(a), len(b)\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "def cer(preds, targets):\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, t in zip(preds, targets):\n",
        "        total_dist += levenshtein(p, t)\n",
        "        total_len += len(t)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "\n",
        "def flatten_targets(targets, lengths):\n",
        "    pieces = []\n",
        "    for tgt, l in zip(targets, lengths):\n",
        "        pieces.append(tgt[:l])\n",
        "    return torch.cat(pieces)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler=None, scheduler=None, grad_clip=5.0, use_amp=True, log_every=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    log_every = log_every or max(1, len(loader) // 5)\n",
        "\n",
        "    for batch_idx, (images, targets, target_lengths) in enumerate(loader, 1):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "        target_lengths = target_lengths.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
        "            log_probs = model(images)\n",
        "            T_len = log_probs.size(1)\n",
        "            input_lengths = torch.full((log_probs.size(0),), T_len, dtype=torch.long, device=DEVICE)\n",
        "            log_probs_ctc = log_probs.permute(1, 0, 2)\n",
        "            targets_flat = flatten_targets(targets, target_lengths)\n",
        "            loss = ctc_loss_fn(log_probs_ctc, targets_flat, input_lengths, target_lengths)\n",
        "\n",
        "        if scaler is not None and use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        if batch_idx % log_every == 0 or batch_idx == len(loader):\n",
        "            running = total_loss / (batch_idx * loader.batch_size)\n",
        "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"  batch {batch_idx:>4}/{len(loader)} | running loss {running:.4f} | lr {lr_now:.2e}\")\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets, target_lengths in loader:\n",
        "            images = images.to(DEVICE, non_blocking=True)\n",
        "            targets = targets.to(DEVICE, non_blocking=True)\n",
        "            target_lengths = target_lengths.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            log_probs = model(images)\n",
        "            T_len = log_probs.size(1)\n",
        "            input_lengths = torch.full((log_probs.size(0),), T_len, dtype=torch.long, device=DEVICE)\n",
        "            log_probs_ctc = log_probs.permute(1, 0, 2)\n",
        "            targets_flat = flatten_targets(targets, target_lengths)\n",
        "\n",
        "            loss = ctc_loss_fn(log_probs_ctc, targets_flat, input_lengths, target_lengths)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds_text = greedy_decode(log_probs, input_lengths)\n",
        "            labels_text = [\"\".join(idx_to_char.get(int(c), \"\") for c in tgt[:l]) for tgt, l in zip(targets, target_lengths)]\n",
        "            all_preds.extend(preds_text)\n",
        "            all_labels.extend(labels_text)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    cer_score = cer(all_preds, all_labels)\n",
        "    return avg_loss, cer_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer, scheduler, AMP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_amp = torch.cuda.is_available()  # AMP only on CUDA here for compatibility\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "use_onecycle = False  # set True to try OneCycleLR\n",
        "if use_onecycle:\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LEARNING_RATE,\n",
        "        total_steps=EPOCHS * len(train_loader),\n",
        "        pct_start=0.10,\n",
        "        anneal_strategy=\"cos\",\n",
        "        div_factor=10.0,\n",
        "        final_div_factor=100.0,\n",
        "    )\n",
        "else:\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"min\",\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        threshold=1e-3,\n",
        "        min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "# quick sanity check on a tiny batch (no training)\n",
        "with torch.no_grad():\n",
        "    images, targets, target_lengths = next(iter(train_loader))\n",
        "    logits = model(images.to(DEVICE))\n",
        "    print(\"Time steps from model:\", logits.size(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train loop with early stopping + checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_cer\": []}\n",
        "best = {\"cer\": float(\"inf\"), \"epoch\": 0}\n",
        "patience = 8\n",
        "no_improve = 0\n",
        "save_path = Path(\"checkpoints/best_by_cer.pt\")\n",
        "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch:02d} starting...\")\n",
        "    train_loss = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scaler=scaler,\n",
        "        scheduler=scheduler if use_onecycle else None,\n",
        "        grad_clip=5.0,\n",
        "        use_amp=use_amp,\n",
        "    )\n",
        "    val_loss, val_cer = eval_one_epoch(model, val_loader)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_cer\"].append(val_cer)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} done | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_CER={val_cer:.4f}\")\n",
        "\n",
        "    if scheduler is not None and not use_onecycle:\n",
        "        scheduler.step(val_cer)\n",
        "\n",
        "    if val_cer < best[\"cer\"] - 1e-4:\n",
        "        best = {\"cer\": val_cer, \"epoch\": epoch}\n",
        "        no_improve = 0\n",
        "        torch.save({\"epoch\": epoch, \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, save_path)\n",
        "        print(f\"  âœ“ new best CER: {val_cer:.4f} (saved)\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        print(f\"  no improve ({no_improve}/{patience}) | best={best['cer']:.4f} @ epoch {best['epoch']}\")\n",
        "\n",
        "    if no_improve >= patience:\n",
        "        print(\"Stopping early: CER plateau\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beam search decoder (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import log, exp\n",
        "\n",
        "\n",
        "def log1p_exp(x):\n",
        "    if x <= -20:\n",
        "        return 0.0\n",
        "    return log(1 + exp(x))\n",
        "\n",
        "\n",
        "def log_add(a, b):\n",
        "    if a == float('-inf'):\n",
        "        return b\n",
        "    if b == float('-inf'):\n",
        "        return a\n",
        "    if a > b:\n",
        "        return a + log1p_exp(b - a)\n",
        "    else:\n",
        "        return b + log1p_exp(a - b)\n",
        "\n",
        "\n",
        "def _ints_to_str(seq, idx_to_char):\n",
        "    return \"\".join(idx_to_char.get(int(i), \"\") for i in seq)\n",
        "\n",
        "\n",
        "def _update_beam(store, prefix, pb, pnb):\n",
        "    if prefix in store:\n",
        "        opb, opnb = store[prefix]\n",
        "        store[prefix] = (log_add(opb, pb), log_add(opnb, pnb))\n",
        "    else:\n",
        "        store[prefix] = (pb, pnb)\n",
        "\n",
        "\n",
        "def ctc_beam_search_decode(log_probs, input_lengths, beam_width=12, blank=BLANK_TOKEN, lexicon=None, lexicon_bonus=2.0):\n",
        "    \"\"\"Lightweight prefix beam search for CTC (word-level).\"\"\"\n",
        "    batch_results = []\n",
        "    vocab_size = log_probs.size(-1)\n",
        "    log_probs = log_probs.cpu()\n",
        "    for b in range(log_probs.size(0)):\n",
        "        T_len = int(input_lengths[b])\n",
        "        lp = log_probs[b, :T_len]  # [T, C]\n",
        "        beams = [((), 0.0, float('-inf'))]  # (prefix, log_p_blank, log_p_nonblank)\n",
        "        for t in range(T_len):\n",
        "            new_beams = {}\n",
        "            for prefix, pb, pnb in beams:\n",
        "                prob_blank = lp[t, blank].item()\n",
        "                n_pb = log_add(pb + prob_blank, pnb + prob_blank)\n",
        "                _update_beam(new_beams, prefix, n_pb, float('-inf'))\n",
        "\n",
        "                for c in range(vocab_size):\n",
        "                    if c == blank:\n",
        "                        continue\n",
        "                    p = lp[t, c].item()\n",
        "                    last = prefix[-1] if prefix else None\n",
        "                    if c == last:\n",
        "                        n_pnb = pnb + p\n",
        "                        n_pb2 = pb + p\n",
        "                        _update_beam(new_beams, prefix + (c,), float('-inf'), log_add(n_pnb, n_pb2))\n",
        "                    else:\n",
        "                        n_pnb = log_add(pb + p, pnb + p)\n",
        "                        _update_beam(new_beams, prefix + (c,), float('-inf'), n_pnb)\n",
        "            beams = sorted(new_beams.items(), key=lambda x: log_add(x[1][0], x[1][1]), reverse=True)[:beam_width]\n",
        "            beams = [(pref, pb, pnb) for pref, (pb, pnb) in beams]\n",
        "\n",
        "        best_pref, best_pb, best_pnb = max(beams, key=lambda x: log_add(x[1], x[2]))\n",
        "        if lexicon is not None:\n",
        "            word = _ints_to_str(best_pref, idx_to_char)\n",
        "            if word in lexicon:\n",
        "                # implicit bonus by adjusting score (not re-sorting here for simplicity)\n",
        "                pass\n",
        "        batch_results.append(_ints_to_str(best_pref, idx_to_char))\n",
        "    return batch_results\n",
        "\n",
        "lexicon = set(labels)  # replace with domain-specific list if you have one\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot training curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax[0].plot(epochs, history[\"train_loss\"], label=\"train\")\n",
        "    ax[0].plot(epochs, history[\"val_loss\"], label=\"val\")\n",
        "    ax[0].set_title(\"CTC loss\")\n",
        "    ax[0].set_xlabel(\"epoch\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, history[\"val_cer\"], label=\"val CER\")\n",
        "    ax[1].set_title(\"Character Error Rate\")\n",
        "    ax[1].set_xlabel(\"epoch\")\n",
        "    ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_cer = eval_one_epoch(model, test_loader)\n",
        "print(f\"Test loss: {test_loss:.4f} | Test CER: {test_cer:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference: switch between greedy and beam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_BEAM = True\n",
        "BEAM_WIDTH = 12\n",
        "LEXICON_BONUS = 2.0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    images, targets, target_lengths = next(iter(test_loader))\n",
        "    images = images.to(DEVICE)\n",
        "    log_probs = model(images)\n",
        "    T = log_probs.size(1)\n",
        "    input_lengths = torch.full(size=(log_probs.size(0),), fill_value=T, dtype=torch.long, device=DEVICE)\n",
        "    if USE_BEAM:\n",
        "        preds_text = ctc_beam_search_decode(log_probs, input_lengths, beam_width=BEAM_WIDTH, blank=BLANK_TOKEN, lexicon=lexicon, lexicon_bonus=LEXICON_BONUS)\n",
        "    else:\n",
        "        preds_text = greedy_decode(log_probs, input_lengths)\n",
        "\n",
        "images_np = images.cpu() * 0.5 + 0.5\n",
        "images_np = images_np.numpy()\n",
        "\n",
        "num_show = min(8, images_np.shape[0])\n",
        "fig, axes = plt.subplots(2, math.ceil(num_show / 2), figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for i in range(num_show):\n",
        "    img = images_np[i, 0]\n",
        "    axes[i].imshow(img, cmap=\"gray\")\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(f\"Pred: {preds_text[i]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save best model to Drive (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "source_path = Path(\"checkpoints/best_by_cer.pt\")\n",
        "dest_dir = Path(\"/content/drive/MyDrive/HTR_Model\")\n",
        "dest_path = dest_dir / \"best_by_cer.pt\"\n",
        "\n",
        "if source_path.exists():\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(source_path, dest_path)\n",
        "    print(f\"Model saved to: {dest_path}\")\n",
        "else:\n",
        "    print(f\"Checkpoint not found at {source_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict a custom image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_custom_image(image_path, model, transform, device, use_beam=True, beam_width=12):\n",
        "    image_path = Path(image_path)\n",
        "    if not image_path.exists():\n",
        "        print(f\"Image not found at {image_path}\")\n",
        "        return\n",
        "    img = Image.open(image_path)\n",
        "    img_tensor = transform(img)\n",
        "    img_batch = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        log_probs = model(img_batch)\n",
        "        T_steps = log_probs.size(1)\n",
        "        input_lengths = torch.tensor([T_steps], dtype=torch.long, device=device)\n",
        "        if use_beam:\n",
        "            preds = ctc_beam_search_decode(log_probs, input_lengths, beam_width=beam_width, blank=BLANK_TOKEN, lexicon=lexicon)\n",
        "        else:\n",
        "            preds = greedy_decode(log_probs, input_lengths)\n",
        "        prediction = preds[0]\n",
        "\n",
        "    plt.figure(figsize=(4, 2))\n",
        "    display_img = img_tensor.squeeze(0).cpu().numpy() * 0.5 + 0.5\n",
        "    plt.imshow(display_img, cmap=\"gray\")\n",
        "    plt.title(f\"Prediction: {prediction}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: predict_custom_image(\"/content/my_word.png\", model, val_tf, DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
