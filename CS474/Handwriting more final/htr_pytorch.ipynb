{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handwritten Text Recognition (PyTorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports: core libs, plotting, and torch bits\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings & hyperparameters\n",
        "# Bump resolution to give the model more signal on messy handwriting\n",
        "IMAGE_SIZE = (160, 48)  # (width, height)\n",
        "BATCH_SIZE = 48        # slightly smaller batch to accommodate bigger images\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "PAD_TOKEN = 99  # for collate padding only; CTC uses blank separately\n",
        "BLANK_TOKEN = 0  # for CTC loss\n",
        "SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data download (IAM words via kagglehub)\n",
        "Set `KAGGLE_API_TOKEN` before running (token supplied separately). This pulls the IAM handwriting word dataset. Skip if you already have the files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /opt/anaconda3/lib/python3.12/site-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (23.2)\n",
            "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.2)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Dataset downloaded to: /Users/grantgardner/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2\n"
          ]
        }
      ],
      "source": [
        "# Download dataset (run once). Requires KAGGLE_API_TOKEN in env.\n",
        "\n",
        "%pip install kagglehub\n",
        "import os\n",
        "from pathlib import Path\n",
        "import kagglehub\n",
        "\n",
        "# Set the KAGGLE_API_TOKEN environment variable provided\n",
        "os.environ['KAGGLE_API_TOKEN'] = 'KGAT_be9c382f7d5391aa23404b90f2c252cb'\n",
        "try:\n",
        "    import kagglehub\n",
        "    iam_root = Path(kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\"))\n",
        "    print(f\"Dataset downloaded to: {iam_root}\")\n",
        "except Exception as e:\n",
        "    print(\"Download skipped/failed; set KAGGLE_API_TOKEN then rerun if needed.\")\n",
        "    iam_root = Path(\"/kaggle/input/iam-handwriting-word-database\")  # fallback default\n",
        "\n",
        "DATA_INPUT_PATH = iam_root\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect image paths, labels, and charset\n",
        "Parses `words.txt`, filters missing images, and builds mappings for CTC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse IAM words metadata\n",
        "labels = []\n",
        "images_path = []\n",
        "\n",
        "words_file = DATA_INPUT_PATH / \"iam_words\" / \"words.txt\"\n",
        "assert words_file.exists(), f\"Missing words.txt at {words_file}\"\n",
        "\n",
        "characters = set()\n",
        "max_len = 0\n",
        "with words_file.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"#\") or not line.strip():\n",
        "            continue\n",
        "        parts = line.strip().split()\n",
        "        word_id = parts[0]\n",
        "        label = parts[-1]\n",
        "\n",
        "        first_folder = word_id.split(\"-\")[0]\n",
        "        second_folder = f\"{first_folder}-{word_id.split('-')[1]}\"\n",
        "        image_filename = f\"{word_id}.png\"\n",
        "        image_path = DATA_INPUT_PATH / \"iam_words\" / \"words\" / first_folder / second_folder / image_filename\n",
        "        if image_path.is_file() and image_path.stat().st_size > 0:\n",
        "            images_path.append(str(image_path))\n",
        "            labels.append(label)\n",
        "            characters.update(label)\n",
        "            max_len = max(max_len, len(label))\n",
        "\n",
        "characters = sorted(list(characters))\n",
        "print(f\"Total samples: {len(images_path)} | Max label len: {max_len}\")\n",
        "print(f\"Charset size: {len(characters)} -> {characters[:20]}{' ...' if len(characters) > 20 else ''}\")\n",
        "\n",
        "# Build lookup tables\n",
        "# Reserve 0 for blank (CTC); start characters at 1.\n",
        "char_to_idx = {c: i + 1 for i, c in enumerate(characters)}\n",
        "idx_to_char = {i + 1: c for i, c in enumerate(characters)}\n",
        "num_classes = len(characters) + 1  # +1 for blank\n",
        "print(f\"num_classes (incl. blank): {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image preprocessing helpers\n",
        "Resize with padding, normalize, and keep grayscale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomGaussianBlur:\n",
        "    def __init__(self, p=0.2, radius=(0.1, 1.2)):\n",
        "        self.p = p\n",
        "        self.radius = radius\n",
        "\n",
        "    def __call__(self, img: Image.Image):\n",
        "        if random.random() > self.p:\n",
        "            return img\n",
        "        r = random.uniform(self.radius[0], self.radius[1])\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=r))\n",
        "\n",
        "\n",
        "class AddGaussianNoise:\n",
        "    def __init__(self, p=0.25, sigma=(0.0, 0.08)):\n",
        "        self.p = p\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x: torch.Tensor):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        s = random.uniform(self.sigma[0], self.sigma[1])\n",
        "        return (x + torch.randn_like(x) * s).clamp(-1.0, 1.0)\n",
        "\n",
        "\n",
        "def build_transforms(image_size_wh):\n",
        "    w, h = image_size_wh  # our IMAGE_SIZE is (W, H)\n",
        "    train_tf = T.Compose([\n",
        "        T.Grayscale(num_output_channels=1),\n",
        "        T.Resize((h, w)),  # torchvision uses (H, W)\n",
        "        T.RandomApply([T.RandomAffine(\n",
        "            degrees=10,\n",
        "            translate=(0.06, 0.12),\n",
        "            scale=(0.85, 1.15),\n",
        "            shear=(-10, 10),\n",
        "            interpolation=T.InterpolationMode.BILINEAR,\n",
        "            fill=255,\n",
        "        )], p=0.75),\n",
        "        T.RandomApply([T.RandomPerspective(distortion_scale=0.20, p=1.0)], p=0.25),\n",
        "        T.RandomApply([RandomGaussianBlur(p=1.0)], p=0.25),\n",
        "        T.RandomApply([T.ColorJitter(brightness=0.25, contrast=0.35)], p=0.60),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5], [0.5]),  # -> [-1,1]\n",
        "        AddGaussianNoise(p=0.30, sigma=(0.0, 0.06)),\n",
        "        T.RandomErasing(p=0.20, scale=(0.01, 0.05), ratio=(0.2, 5.0), value=1.0),\n",
        "    ])\n",
        "\n",
        "    val_tf = T.Compose([\n",
        "        T.Grayscale(num_output_channels=1),\n",
        "        T.Resize((h, w)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5], [0.5]),\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "train_tf, val_tf = build_transforms(IMAGE_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset + collate\n",
        "Loads images/labels and preps them for CTC (keeps lengths).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IAMWordDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        target = torch.tensor([char_to_idx[c] for c in label], dtype=torch.long)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n",
        "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN)\n",
        "    return images, targets_padded, target_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/val/test split + loaders\n",
        "Simple 80/10/10 split; tweak `test_size` if you want more validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_imgs, test_imgs, train_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      2\u001b[0m     images_path, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED, stratify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m val_imgs, test_imgs, val_labels, test_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      5\u001b[0m     test_imgs, test_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED, stratify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m IAMWordDataset(train_imgs, train_labels, transform\u001b[38;5;241m=\u001b[39mpreprocess_transform)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "train_imgs, test_imgs, train_labels, test_labels = train_test_split(\n",
        "    images_path, labels, test_size=0.2, random_state=SEED, stratify=None\n",
        ")\n",
        "val_imgs, test_imgs, val_labels, test_labels = train_test_split(\n",
        "    test_imgs, test_labels, test_size=0.5, random_state=SEED, stratify=None\n",
        ")\n",
        "\n",
        "train_ds = IAMWordDataset(train_imgs, train_labels, transform=train_tf)\n",
        "val_ds = IAMWordDataset(val_imgs, val_labels, transform=val_tf)\n",
        "test_ds = IAMWordDataset(test_imgs, test_labels, transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_batch)\n",
        "\n",
        "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model: CNN feature extractor + BiLSTM + CTC head\n",
        "Basic, readable architecture tuned for word-level recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HTRModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),  # 64x16\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),  # 32x8\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d((2, 1), (2, 1)),  # squeeze height a bit\n",
        "        )\n",
        "        # After convs: [B, C=128, H=?, W=?]\n",
        "        # We collapse H dimension and treat W as time steps.\n",
        "        rnn_input_size = 128 * (IMAGE_SIZE[1] // 8)\n",
        "        self.lstm1 = nn.LSTM(rnn_input_size, 256, bidirectional=True, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
        "        self.classifier = nn.Linear(256, num_classes)  # 2*128 from bi-LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # [B, C, H, W]\n",
        "        b, c, h, w = x.size()\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()  # [B, W, C, H]\n",
        "        x = x.view(b, w, c * h)  # [B, W, C*H]\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.classifier(x)  # [B, W, num_classes]\n",
        "        x = x.log_softmax(2)  # log-probs for CTC\n",
        "        return x\n",
        "\n",
        "model = HTRModel(num_classes=num_classes).to(DEVICE)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training helpers\n",
        "CTC loss, greedy decode, CER/Levenshtein, and one-epoch train/val loops.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ctc_loss_fn = nn.CTCLoss(blank=BLANK_TOKEN, zero_infinity=True)\n",
        "\n",
        "\n",
        "def greedy_decode(log_probs, input_lengths):\n",
        "    \"\"\"Greedy CTC decode -> list of strings.\"\"\"\n",
        "    max_probs = log_probs.detach().cpu().argmax(2)  # [B, T]\n",
        "    decoded = []\n",
        "    for seq, seq_len in zip(max_probs, input_lengths):\n",
        "        prev = None\n",
        "        chars = []\n",
        "        for idx in seq[:seq_len]:\n",
        "            idx = idx.item()\n",
        "            if idx == BLANK_TOKEN:\n",
        "                prev = None\n",
        "                continue\n",
        "            if idx != prev:  # collapse repeats\n",
        "                chars.append(idx_to_char.get(idx, \"\"))\n",
        "            prev = idx\n",
        "        decoded.append(\"\".join(chars))\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    \"\"\"Simple Levenshtein distance for CER.\"\"\"\n",
        "    m, n = len(a), len(b)\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "    return dp[m][n]\n",
        "\n",
        "\n",
        "def cer(preds, targets):\n",
        "    \"\"\"Character error rate across batch.\"\"\"\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, t in zip(preds, targets):\n",
        "        total_dist += levenshtein(p, t)\n",
        "        total_len += len(t)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "\n",
        "def flatten_targets(targets, lengths):\n",
        "    \"\"\"Remove padding for CTC target format.\"\"\"\n",
        "    pieces = []\n",
        "    for tgt, l in zip(targets, lengths):\n",
        "        pieces.append(tgt[:l])\n",
        "    return torch.cat(pieces)\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler=None, scheduler=None, grad_clip=5.0, use_amp=True, log_every=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    log_every = log_every or max(1, len(loader) // 5)\n",
        "\n",
        "    for batch_idx, (images, targets, target_lengths) in enumerate(loader, 1):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.to(DEVICE, non_blocking=True)\n",
        "        target_lengths = target_lengths.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # torch.amp.autocast is the modern API; device_type='cuda' is required when enabled\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
        "            log_probs = model(images)  # [B, T, C] (already log_softmax)\n",
        "            T_len = log_probs.size(1)\n",
        "            input_lengths = torch.full((log_probs.size(0),), T_len, dtype=torch.long, device=DEVICE)\n",
        "            log_probs_ctc = log_probs.permute(1, 0, 2)  # [T, B, C]\n",
        "            targets_flat = flatten_targets(targets, target_lengths)\n",
        "            loss = ctc_loss_fn(log_probs_ctc, targets_flat, input_lengths, target_lengths)\n",
        "\n",
        "        if scaler is not None and use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        if batch_idx % log_every == 0 or batch_idx == len(loader):\n",
        "            running = total_loss / (batch_idx * loader.batch_size)\n",
        "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"  batch {batch_idx:>4}/{len(loader)} | running loss {running:.4f} | lr {lr_now:.2e}\")\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets, target_lengths in loader:\n",
        "            images = images.to(DEVICE, non_blocking=True)\n",
        "            targets = targets.to(DEVICE, non_blocking=True)\n",
        "            target_lengths = target_lengths.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            log_probs = model(images)\n",
        "            T_len = log_probs.size(1)\n",
        "            input_lengths = torch.full((log_probs.size(0),), T_len, dtype=torch.long, device=DEVICE)\n",
        "            log_probs_ctc = log_probs.permute(1, 0, 2)\n",
        "            targets_flat = flatten_targets(targets, target_lengths)\n",
        "\n",
        "            loss = ctc_loss_fn(log_probs_ctc, targets_flat, input_lengths, target_lengths)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds_text = greedy_decode(log_probs, input_lengths)\n",
        "            labels_text = [\"\".join(idx_to_char.get(int(c), \"\") for c in tgt[:l]) for tgt, l in zip(targets, target_lengths)]\n",
        "            all_preds.extend(preds_text)\n",
        "            all_labels.extend(labels_text)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    cer_score = cer(all_preds, all_labels)\n",
        "    return avg_loss, cer_score\n",
        "\n",
        "use_amp = torch.cuda.is_available()  # AMP only on CUDA here for compatibility\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "use_onecycle = False  # set True to try OneCycleLR\n",
        "if use_onecycle:\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LEARNING_RATE,\n",
        "        total_steps=EPOCHS * len(train_loader),\n",
        "        pct_start=0.10,\n",
        "        anneal_strategy=\"cos\",\n",
        "        div_factor=10.0,\n",
        "        final_div_factor=100.0,\n",
        "    )\n",
        "else:\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"min\",\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        threshold=1e-3,\n",
        "        min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "# quick sanity check on a tiny batch (no training)\n",
        "with torch.no_grad():\n",
        "    images, targets, target_lengths = next(iter(train_loader))\n",
        "    logits = model(images.to(DEVICE))\n",
        "    print(\"Time steps from model:\", logits.size(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train loop\n",
        "Tracks train/val loss and CER; early-stop-like patience is easy to add.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_cer\": []}\n",
        "best = {\"cer\": float(\"inf\"), \"epoch\": 0}\n",
        "patience = 8\n",
        "no_improve = 0\n",
        "save_path = Path(\"checkpoints/best_by_cer.pt\")\n",
        "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch:02d} starting...\")\n",
        "    train_loss = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        optimizer,\n",
        "        scaler=scaler,\n",
        "        scheduler=scheduler if use_onecycle else None,\n",
        "        grad_clip=5.0,\n",
        "        use_amp=use_amp,\n",
        "    )\n",
        "    val_loss, val_cer = eval_one_epoch(model, val_loader)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_cer\"].append(val_cer)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} done | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_CER={val_cer:.4f}\")\n",
        "\n",
        "    if scheduler is not None and not use_onecycle:\n",
        "        scheduler.step(val_cer)  # ReduceLROnPlateau monitors CER\n",
        "\n",
        "    if val_cer < best[\"cer\"] - 1e-4:\n",
        "        best = {\"cer\": val_cer, \"epoch\": epoch}\n",
        "        no_improve = 0\n",
        "        torch.save({\"epoch\": epoch, \"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, save_path)\n",
        "        print(f\"  âœ“ new best CER: {val_cer:.4f} (saved)\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        print(f\"  no improve ({no_improve}/{patience}) | best={best['cer']:.4f} @ epoch {best['epoch']}\")\n",
        "\n",
        "    if no_improve >= patience:\n",
        "        print(\"Stopping early: CER plateau\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot training curves\n",
        "Loss + CER to spot overfitting trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax[0].plot(epochs, history[\"train_loss\"], label=\"train\")\n",
        "    ax[0].plot(epochs, history[\"val_loss\"], label=\"val\")\n",
        "    ax[0].set_title(\"CTC loss\")\n",
        "    ax[0].set_xlabel(\"epoch\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, history[\"val_cer\"], label=\"val CER\")\n",
        "    ax[1].set_title(\"Character Error Rate\")\n",
        "    ax[1].set_xlabel(\"epoch\")\n",
        "    ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on test set\n",
        "Final CER + quick loss check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_cer = eval_one_epoch(model, test_loader)\n",
        "print(f\"Test loss: {test_loss:.4f} | Test CER: {test_cer:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beam search decoding (optional) with lexicon bias\n",
        "Greedy works, but beam search plus a wordlist can squeeze out extra CER gains. Set a small lexicon (e.g., the IAM label set) and tune beam width / lexicon weight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import log, exp\n",
        "\n",
        "\n",
        "def ctc_beam_search_decode(log_probs, input_lengths, beam_width=10, blank=BLANK_TOKEN, lexicon=None, lexicon_bonus=2.0):\n",
        "    \"\"\"\n",
        "    Simple prefix beam search for CTC. log_probs: [B, T, C] (log-softmax). Returns list of strings.\n",
        "    lexicon: optional set of valid words; if provided, sequences that land in lexicon get a bonus.\n",
        "    lexicon_bonus: log-space additive bonus applied when a completed word is in lexicon.\n",
        "    \"\"\"\n",
        "    # This is a lightweight implementation for word-level outputs; for production consider pyctcdecode/kenlm.\n",
        "    batch_results = []\n",
        "    vocab_size = log_probs.size(-1)\n",
        "    log_probs = log_probs.cpu()\n",
        "    for b in range(log_probs.size(0)):\n",
        "        T_len = int(input_lengths[b])\n",
        "        lp = log_probs[b, :T_len]  # [T, C]\n",
        "        # Beams: list of (prefix(tuple of ints), log_p_blank, log_p_nonblank)\n",
        "        beams = [((), 0.0, float('-inf'))]  # start with empty prefix\n",
        "        for t in range(T_len):\n",
        "            new_beams = {}\n",
        "            for prefix, pb, pnb in beams:\n",
        "                # stay at blank\n",
        "                prob_blank = lp[t, blank].item()\n",
        "                n_pb = log_add(pb + prob_blank, pnb + prob_blank)\n",
        "                _update_beam(new_beams, prefix, n_pb, float('-inf'))\n",
        "\n",
        "                # extend with symbols\n",
        "                for c in range(vocab_size):\n",
        "                    if c == blank:\n",
        "                        continue\n",
        "                    p = lp[t, c].item()\n",
        "                    last = prefix[-1] if prefix else None\n",
        "                    if c == last:\n",
        "                        # repeat char\n",
        "                        n_pnb = pnb + p  # must come from non-blank\n",
        "                        n_pb = pb + p    # or from blank->nonblank transition? we keep only nonblank here\n",
        "                        _update_beam(new_beams, prefix + (c,), float('-inf'), log_add(n_pnb, n_pb))\n",
        "                    else:\n",
        "                        n_pnb = log_add(pb + p, pnb + p)\n",
        "                        _update_beam(new_beams, prefix + (c,), float('-inf'), n_pnb)\n",
        "            # prune\n",
        "            beams = sorted(new_beams.items(), key=lambda x: log_add(x[1][0], x[1][1]), reverse=True)[:beam_width]\n",
        "            beams = [(pref, pb, pnb) for pref, (pb, pnb) in beams]\n",
        "\n",
        "        # pick best\n",
        "        best_pref, best_pb, best_pnb = max(beams, key=lambda x: log_add(x[1], x[2]))\n",
        "        best_logp = log_add(best_pb, best_pnb)\n",
        "\n",
        "        # optional lexicon bias\n",
        "        if lexicon is not None:\n",
        "            word = _ints_to_str(best_pref, idx_to_char)\n",
        "            if word in lexicon:\n",
        "                best_logp += lexicon_bonus\n",
        "        batch_results.append(_ints_to_str(best_pref, idx_to_char))\n",
        "    return batch_results\n",
        "\n",
        "\n",
        "def _update_beam(store, prefix, pb, pnb):\n",
        "    if prefix in store:\n",
        "        opb, opnb = store[prefix]\n",
        "        store[prefix] = (log_add(opb, pb), log_add(opnb, pnb))\n",
        "    else:\n",
        "        store[prefix] = (pb, pnb)\n",
        "\n",
        "\n",
        "def log_add(a, b):\n",
        "    if a == float('-inf'):\n",
        "        return b\n",
        "    if b == float('-inf'):\n",
        "        return a\n",
        "    if a > b:\n",
        "        return a + log1p_exp(b - a)\n",
        "    else:\n",
        "        return b + log1p_exp(a - b)\n",
        "\n",
        "\n",
        "def log1p_exp(x):\n",
        "    if x <= -20:\n",
        "        return 0.0\n",
        "    return log(1 + exp(x))\n",
        "\n",
        "\n",
        "def _ints_to_str(seq, idx_to_char):\n",
        "    return \"\".join(idx_to_char.get(int(i), \"\") for i in seq)\n",
        "\n",
        "# Optionally build a lexicon from labels (distinct words)\n",
        "lexicon = set(labels)  # you can swap this with a tighter domain-specific wordlist\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference and sample viz\n",
        "Switch between greedy and beam+lexicon decoding to see impact on outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_BEAM = True\n",
        "BEAM_WIDTH = 12\n",
        "LEXICON_BONUS = 2.0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    images, targets, target_lengths = next(iter(test_loader))\n",
        "    images = images.to(DEVICE)\n",
        "    log_probs = model(images)\n",
        "    T = log_probs.size(1)\n",
        "    input_lengths = torch.full(size=(log_probs.size(0),), fill_value=T, dtype=torch.long, device=DEVICE)\n",
        "    if USE_BEAM:\n",
        "        preds_text = ctc_beam_search_decode(log_probs, input_lengths, beam_width=BEAM_WIDTH, blank=BLANK_TOKEN, lexicon=lexicon, lexicon_bonus=LEXICON_BONUS)\n",
        "    else:\n",
        "        preds_text = greedy_decode(log_probs, input_lengths)\n",
        "\n",
        "# convert images back to displayable format\n",
        "images_np = images.cpu() * 0.5 + 0.5  # undo normalization\n",
        "images_np = images_np.numpy()\n",
        "\n",
        "num_show = min(8, images_np.shape[0])\n",
        "fig, axes = plt.subplots(2, math.ceil(num_show / 2), figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for i in range(num_show):\n",
        "    img = images_np[i, 0]\n",
        "    axes[i].imshow(img, cmap=\"gray\")\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(f\"Pred: {preds_text[i]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model\n",
        "Stores only the prediction-friendly weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_DIR = Path(\"./artifacts\")\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "model_path = MODEL_DIR / \"htr_pytorch.pth\"\n",
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"char_to_idx\": char_to_idx,\n",
        "    \"idx_to_char\": idx_to_char,\n",
        "    \"image_size\": IMAGE_SIZE,\n",
        "}, model_path)\n",
        "print(f\"Saved to {model_path.resolve()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
